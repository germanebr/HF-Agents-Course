{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base code follows the steps required to finetune a LLM through the **LoRA** method.\n",
    "\n",
    "The overall process takes approximately 6 hours with a basic GPU unit, so the code is only for reference in case no GPU is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "Make sure that your environment has the following packages installed to work properly:\n",
    "\n",
    "* *bitsandbytes* for quantization\n",
    "* *peft* for implementing LoRA\n",
    "* *Transformers* for loading the HuggingFace models\n",
    "* *datasets* for loading and using the fine-tuned dataset\n",
    "* *trl* for the trainer class\n",
    "\n",
    "*pip install bitsandbytes peft transformers trl tensorboardX wandb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Hugging Face token to push the model into the HF Hub\n",
    "\n",
    "Make sure to have a Hugging Face token with a **write role** to be able to push the code into the Hub. Also, make sure to use that token as an environment variable as *HF_TOKEN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GReyes15\\OneDrive - JNJ\\Desktop\\Git\\HF-Agents-Course\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    hf_token = f.readline()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the dataset into inputs\n",
    "\n",
    "It is necessary to prepare the training set into the format that the LLM requires to make sure it learns the format we desire.\n",
    "\n",
    "This sample will use the dataset for function calling *NousResearch/hermes-function-calling-v1* by adding some thinking processes from **deepseek-ai/DeepSeek-R1-Distill-Qwen-32B**.\n",
    "\n",
    "The importance of processing the data resides in the model requiring a specific data format to properly follow the conversation with the user, as well as to know when to call the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "dataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    first_message = messages[0]\n",
    "\n",
    "    # Instead of adding a system message, we merge the content into the first user message\n",
    "    if first_message[\"role\"] == \"system\":\n",
    "        system_message_content = first_message[\"content\"]\n",
    "\n",
    "        # Merge system content with the first user message\n",
    "        messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function, take the time to plan the function to take. Make that thinking process between <think>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
    "\n",
    "        # Remove the system message from the conversation\n",
    "        messages.pop(0)\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize = False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for finetuning\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversations\", \"messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Dedicated Dataset for this Unit\n",
    "\n",
    "The *NousResearch/hermes-funciton-calling-v1* dataset is considered a reference for finetuning LLMs. However, it doesn't have a **thinking** step in the messages of the dataset.\n",
    "\n",
    "It has been demonstrated that giving the thinking time to the LLM before generating an answer or performing an action significantly improves the performance and quality of the responses.\n",
    "\n",
    "In this case, the sample dataset that is being used takes as reference the aforementioned dataset, but it was previously given to DeepSeek-R1 to include some *\\<think\\>* tokens before any function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns = \"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the inputs\n",
    "\n",
    "The following lines of code will show how the input messages will look like. In general, the conversations will have the following structure:\n",
    "\n",
    "* A **user message** containing the necessary information with the list of available tools between the *\\<tools\\> \\</tools\\>* tokens, followed by the user query.\n",
    "* An **assistant message** (model) that works with two phases: the *thinking* phase contained in the *\\<think\\> \\</think\\>* tokens, and an *act* phase contained in the *\\<tool_call\\> \\</tool_call\\>* tokens.\n",
    "* In the case the model has a *\\<tools_call\\>* token, we will append the results of that particular action in a new **Tool** message between the *\\<tool_response\\> \\</tool_response\\>* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the tokens used for the model (sanity check)\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the tokenizer\n",
    "\n",
    "By default, the tokenizer splits the given text into sub-words (tokens). However, this is not the behavior we're looking for the new included tokens.\n",
    "\n",
    "Even though we have included the *\\<think\\>*, *\\<tool_call\\>*, and *\\<tool_response\\>* on the dataset, we still need to indicate the tokenizer to treat them as tokens.\n",
    "\n",
    "Finally, we need to modify our chat_template in the tokenizer to reflect those changes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eotool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          pad_token = ChatmlSpecialTokens.pad_token.value,\n",
    "                                          additional_special_tokens = ChatmlSpecialTokens.list())\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             attn_implementation = \"eager\",\n",
    "                                             device_map = \"auto\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the LoRA\n",
    "\n",
    "This block is in charge of defining the parameters of the adapter we'll add and train to the LLM. We need to define the size and importance of the adapters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_dimension: for LoRA update amtrices (smaller = more compression)\n",
    "rank_dimension = 16\n",
    "\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 64\n",
    "\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(r = rank_dimension,\n",
    "                         lora_alpha = lora_alpha,\n",
    "                         lora_dropout = lora_dropout,\n",
    "                         target_modules = [\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # specify the layers in the transformer that will be targeted for training\n",
    "                         task_type = TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Trainer and Fine-Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_username = \"germanebr\"\n",
    "\n",
    "# The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\n",
    "output_dir = \"gemma-2-2B-it-thinking-function_calling-V0\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 5\n",
    "learning_rate = 1e-4 # The initial learning rate for the optimizer.\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs=1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 1500\n",
    "\n",
    "training_arguments = SFTConfig(output_dir = output_dir,\n",
    "                               per_device_train_batch_size = per_device_train_batch_size,\n",
    "                               per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "                               gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                               save_strategy = \"no\",\n",
    "                               eval_strategy = \"epoch\",\n",
    "                               logging_steps = logging_steps,\n",
    "                               learning_rate = learning_rate,\n",
    "                               max_grad_norm = max_grad_norm,\n",
    "                               weight_decay = 0.1,\n",
    "                               warmup_ratio = warmup_ratio,\n",
    "                               lr_scheduler_type = lr_scheduler_type,\n",
    "                               report_to = \"tensorboard\",\n",
    "                               bf16 = True,\n",
    "                               hub_private_repo = False,\n",
    "                               push_to_hub = False,\n",
    "                               num_train_epochs = num_train_epochs,\n",
    "                               gradient_checkpointing = True,\n",
    "                               gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "                               packing = True,\n",
    "                               max_seq_length = max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with a Supervised Fine-Tuning Trainer (SFTTrainer)\n",
    "trainer = SFTTrainer(model = model,\n",
    "                     args = training_arguments,\n",
    "                     train_dataset = dataset[\"train\"],\n",
    "                     eval_dataset = dataset[\"test\"],\n",
    "                     processing_class = tokenizer,\n",
    "                     peft_config = peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the New Model and the Tokenizer to the HF Hub\n",
    "\n",
    "The model will be pushed under the given HuggingFace username + the output_dir we specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(f\"{hf_username}/{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token = \"<eos>\"\n",
    "# push the tokenizer to hub ( replace with your username and your previously specified\n",
    "tokenizer.push_to_hub(f\"{hf_username}/{output_dir}\", token = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "\n",
    "In order to test it, we need to follow the next steps:\n",
    "\n",
    "1. Load the adapter from the hub\n",
    "2. Load the base model: \"**google/gemma-2-2b-it**\" from the hub\n",
    "3. Resize the model to with the new tokens we introduced during the finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit = True,\n",
    "                                bnb_4bit_quant_type = \"nf4\",\n",
    "                                bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant = True)\n",
    "\n",
    "peft_model_id = f\"{hf_username}/{output_dir}\" # replace with your newly trained adapter\n",
    "device = \"auto\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.to(torch.bfloat16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"test\"][8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt is a sub-sample of one of the test set examples. In this example we start the generation after the model generation starts.\n",
    "prompt=\"\"\"<bos><start_of_turn>human\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{tool_call}\n",
    "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
    "\n",
    "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "<think>\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens = 300,# Adapt as necessary\n",
    "                         do_sample = True,\n",
    "                         top_p = 0.95,\n",
    "                         temperature = 0.01,\n",
    "                         repetition_penalty = 1.0,\n",
    "                         eos_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
